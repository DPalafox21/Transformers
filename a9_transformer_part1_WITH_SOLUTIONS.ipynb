{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "567c8c78-ca40-4e15-b209-5919c212d816"
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 9: Transformer and Transformer-based Models\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dd8f442-a6df-4b75-8a33-68092e483c40",
    "outputId": "94a1ed3b-5a17-4fb3-d877-eeb7c06fb92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8609f3cf-1550-46b4-932e-c7a81191042e"
   },
   "source": [
    "## Task 1. Implement the multiple head attention sub layer\n",
    "**Points: 5**\n",
    "\n",
    "### 1.1 Initialize input data\n",
    "Step 1, generate some random input data in the shape of $\\text{n\\_inputs}\\times \\text{d\\_model}$. *Hint*: Use `np.random.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "087a8163-c41b-40b1-86ee-fe4ba8a967d8"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # Do not remove this line\n",
    "\n",
    "d_model = 512\n",
    "n_inputs = 3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "x = np.random.rand(n_inputs,d_model)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41a5c43b-4f12-445f-bc9e-eef841dd206e",
    "outputId": "25a9bfc4-99ce-46e7-f146-1f011f1a3568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\n",
      " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\n",
      " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\n",
      "x.shape: (3, 512)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('x:', x)\n",
    "print('x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db24cce1-8f70-4944-ba17-36f45a62a1a4"
   },
   "source": [
    "**Expected output**\\\n",
    "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\\\n",
    " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\\\n",
    " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\\\n",
    "x.shape: (3, 512)\n",
    "\n",
    "---\n",
    "### 1.2 Create weight matrices for *query*, *key*, and *value*\n",
    "\n",
    "Step 2, create the weight matrices into the correct dimensions. \n",
    "\n",
    "Let's start with `W_query` and `Q`. *Hint*: We first initialize an empty tensor `W` in the dimension of `(d_model, d_k)`, using the `torch.empty()` function. Then we initialize it with `nn.init.xavier_uniform_()`.\n",
    "\n",
    "After `W_query` is initialized, we can get the query matrix `Q` with a multiplication between `x` and `W_query`. *Hint*: Use `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "2417e097-dc26-4560-aadb-6cf6538a5d8b"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty(d_model,d_k) #Create an empty tensor W with the correct dimension.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W) # Randomly initialize the values in the tensor.\n",
    "W_query = W.data.numpy() # Copy out the numpy array\n",
    "\n",
    "### START YOUR CODE ###\n",
    "Q = np.matmul(x,W_query)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e37723c-b393-4447-a33e-3de448f6ab55",
    "outputId": "3ad66abd-f4cc-4033-c851-5d7ee8b8ff03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\n",
      "W_query.shape: (512, 64)\n",
      "Q[0, :5]: [-0.22772416  0.48167867  1.48693414 -1.00410582  0.19323682]\n",
      "Q.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('W_query[0,:5]:', W_query[0,:5])\n",
    "print('W_query.shape:', W_query.shape)\n",
    "print('Q[0, :5]:', Q[0,:5])\n",
    "print('Q.shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46ab6588-bf20-4ec2-9e6f-31d31a8a1607"
   },
   "source": [
    "**Expected output**\\\n",
    "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\\\n",
    "W_query.shape: (512, 64)\\\n",
    "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\\\n",
    "Q.shape: (3, 64)\n",
    "\n",
    "---\n",
    "Next, repeat for `W_key` & `K`, and `W_value` & `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "f56e2283-ef60-440c-9b7e-ce21a9eede97"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty(d_model,d_k)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_key = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "K = np.matmul(x,W_key)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "33f747fd-2e13-4bbb-b59d-c5b0abec134b"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty(d_model,d_k)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_value = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "V = np.matmul(x,W_value)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6fba58b-6162-42e1-a17b-fa7ddb1ca423",
    "outputId": "e7237890-ecd4-4e33-c89d-a3cdf5dbfac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K[0,:5] [ 0.22836541 -0.65482718 -0.07202062  0.49886369  0.5704503 ]\n",
      "K.shape (3, 64)\n",
      "V[0,:5] [-0.44997758  0.92097353 -0.76932045  0.03289758 -0.49462581]\n",
      "V.shape (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('K[0,:5]', K[0,:5])\n",
    "print('K.shape', K.shape)\n",
    "print('V[0,:5]', V[0,:5])\n",
    "print('V.shape', V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82e3f72c-5fae-41e3-b1aa-f8247f691694"
   },
   "source": [
    "**Expected output**\\\n",
    "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\\\n",
    "K.shape (3, 64)\\\n",
    "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\\\n",
    "V.shape (3, 64)\n",
    "\n",
    "---\n",
    "### 1.3 Compute attention scores and weighted output\n",
    "\n",
    "Step 3, compute the attension scores using the matrices `Q` and `K`, following the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "\n",
    "in which $\\sqrt{d_k}$ is for normalization purpose.\n",
    "\n",
    "*Hint*: You should first compute `attn_scores`, which is the unnormalized score. Then you can apply the `softmax()` function imported from `scipy` to calculate the normalized scores. Note that you need to specify the `axis` argument correctly when you call `softmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "77b2bf0c-b3d2-4598-affd-b041cc855348"
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "attn_scores = np.matmul(Q,K.T)/((d_k)**0.5)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "### START YOUR CODE ###\n",
    "attn_scores_norm = softmax(attn_scores,axis=1)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbca70a3-3b71-4c21-a8e0-d28dfe1eb30e",
    "outputId": "d53c8954-ce80-4f2e-c6d9-8af7f382ce4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores.shape: (3, 3)\n",
      "Unnormalized attn_scores: [[-0.75497316 -0.97036241 -0.85112739]\n",
      " [ 0.2377701  -0.70730389 -0.37639248]\n",
      " [ 0.21608568 -0.73905382 -0.89881122]]\n",
      "Normalized atten_scores: [[0.36838498 0.29700213 0.33461289]\n",
      " [0.51820328 0.20140013 0.2803966 ]\n",
      " [0.58387084 0.22464925 0.19147991]]\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('attn_scores.shape:', attn_scores.shape)\n",
    "print('Unnormalized attn_scores:', attn_scores)\n",
    "print('Normalized atten_scores:', attn_scores_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "762525c0-8837-4713-8957-7b8a3d8cea8c"
   },
   "source": [
    "**Expected output**\\\n",
    "attn_scores.shape: (3, 3)\\\n",
    "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\\\n",
    " [ 0.23777018 -0.70730381 -0.37639239]\\\n",
    " [ 0.21608578 -0.73905372 -0.89881112]]\\\n",
    "Normalized atten_scores: [[0.36838498 0.29700212 0.33461289]\\\n",
    " [0.51820328 0.20140013 0.2803966 ]\\\n",
    " [0.58387084 0.22464925 0.19147991]]\\\n",
    "\n",
    "---\n",
    "\n",
    "Step 4, finally, compute the output as the weighted sum of value (`V`), using the above computed `attn_scores_norm` as the weight.\n",
    "\n",
    "*Hint*: `attn_scores_norm[0,:]` is the weight for the first output `weighted_output[0,:]`, \\\n",
    "so the computation is:\\\n",
    "`weighted_output[0,:] = attn_scores_norm[0,0] * V[0,:] + attn_scores_norm[0,1] * V[1,:] + attn_scores_norm[0,2] * V[2,:]`. \\\n",
    "But you can achieve this with one line code using `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4adf620e-2c6e-48d3-a59f-75151d743e38",
    "outputId": "2392692c-02a7-4d8e-b20f-589d46bc6a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_output[0,:5]: [-0.37040035  0.49331394 -0.78595571  0.09711597 -0.33551546]\n",
      "weighted_output.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "weighted_output = np.matmul(attn_scores_norm,V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('weighted_output[0,:5]:', weighted_output[0,:5])\n",
    "print('weighted_output.shape:', weighted_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSfPEhum6TLV"
   },
   "source": [
    "**Expected output**\\\n",
    "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\\\n",
    "weighted_output.shape: (3, 64)\n",
    "\n",
    "---\n",
    "**Congratulation!** You have finished Task 1, and now you know how to implement the self-attention module, which is the core technique of Transformer."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "a9_transformer_part1_WITH_SOLUTIONS.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
